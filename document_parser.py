"""
æ–‡æ¡£è§£ææ¨¡å— - è°ƒç”¨MinerU API
"""
import requests
import os
import time
import json
import zipfile
import tempfile
from io import BytesIO

from config import MINERU_API_URL, MINERU_API_TOKEN


class DocumentParser:
    def __init__(self):
        self.headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {MINERU_API_TOKEN}"
        }

    def parse(self, file_path: str = None, file_url: str = None) -> dict:
        """
        è§£ææ–‡æ¡£å¹¶è¿”å›ç»“æœ

        Args:
            file_path: æœ¬åœ°æ–‡ä»¶è·¯å¾„
            file_url: æ–‡ä»¶URLï¼ˆäºŒé€‰ä¸€ï¼‰

        Returns:
            è§£æç»“æœå­—å…¸ï¼ŒåŒ…å«markdownå†…å®¹å’Œä½ç½®ä¿¡æ¯
        """
        print(f"ğŸ“„ å¼€å§‹è§£ææ–‡æ¡£...")

        if file_url:
            # é€šè¿‡URLç›´æ¥è§£æ
            task_id = self._create_task_by_url(file_url)
            if not task_id:
                raise ValueError("åˆ›å»ºä»»åŠ¡å¤±è´¥")
            print(f"ğŸ“‹ ä»»åŠ¡ID: {task_id}")
            return self._poll_task(task_id)

        elif file_path:
            # æœ¬åœ°æ–‡ä»¶éœ€è¦å…ˆä¸Šä¼ 
            batch_id = self._upload_file(file_path)
            if not batch_id:
                raise ValueError("æ–‡ä»¶ä¸Šä¼ å¤±è´¥")
            print(f"ğŸ“‹ æ‰¹æ¬¡ID: {batch_id}")
            return self._poll_batch(batch_id)

        else:
            raise ValueError("å¿…é¡»æä¾›file_pathæˆ–file_url")

    def _create_task_by_url(self, file_url: str) -> str:
        """é€šè¿‡URLåˆ›å»ºè§£æä»»åŠ¡"""
        url = "https://mineru.net/api/v4/extract/task"
        payload = {
            "url": file_url,
            "is_ocr": True,
            "enable_formula": True,
            "enable_table": True,
            "model_version": "vlm"
        }

        response = requests.post(url, headers=self.headers, json=payload)
        result = response.json()

        print(f"åˆ›å»ºä»»åŠ¡å“åº”: code={result.get('code')}, msg={result.get('msg')}")

        if result.get("code") == 0:
            return result.get("data", {}).get("task_id")

        print(f"åˆ›å»ºä»»åŠ¡å¤±è´¥: {result}")
        return None

    def _upload_file(self, file_path: str) -> str:
        """ä¸Šä¼ æœ¬åœ°æ–‡ä»¶"""
        url = "https://mineru.net/api/v4/file-urls/batch"
        filename = os.path.basename(file_path)

        payload = {
            "files": [{"name": filename}],
            "is_ocr": True,
            "enable_formula": True,
            "enable_table": True,
            "model_version": "vlm"
        }

        import sys
        print(f"ğŸ”‘ å½“å‰Token: {MINERU_API_TOKEN[:30] if MINERU_API_TOKEN else '(æœªè®¾ç½®)'}...", flush=True)
        sys.stdout.flush()

        response = requests.post(url, headers=self.headers, json=payload)

        # æ·»åŠ è°ƒè¯•ä¿¡æ¯
        print(f"APIå“åº”çŠ¶æ€ç : {response.status_code}", flush=True)
        print(f"APIå“åº”å†…å®¹: {response.text[:500] if response.text else '(ç©º)'}", flush=True)
        sys.stdout.flush()

        if not response.text:
            raise ValueError(f"MinerU APIè¿”å›ç©ºå“åº”ï¼ŒçŠ¶æ€ç : {response.status_code}ã€‚è¯·æ£€æŸ¥MINERU_API_TOKENæ˜¯å¦æ­£ç¡®ã€‚")

        result = response.json()

        print(f"ç”³è¯·ä¸Šä¼ URLå“åº”: code={result.get('code')}, msg={result.get('msg')}")

        if result.get("code") != 0:
            print(f"ç”³è¯·ä¸Šä¼ URLå¤±è´¥: {result}")
            return None

        batch_id = result.get("data", {}).get("batch_id")
        file_urls = result.get("data", {}).get("file_urls", [])

        if not file_urls:
            print("æœªè·å–åˆ°ä¸Šä¼ URL")
            return None

        upload_url = file_urls[0]
        print(f"ğŸ“¤ ä¸Šä¼ æ–‡ä»¶åˆ°: {upload_url[:50]}...")

        # ä¸Šä¼ æ–‡ä»¶ï¼ˆæ³¨æ„ï¼šä¸éœ€è¦è®¾ç½®Content-Typeï¼‰
        with open(file_path, "rb") as f:
            upload_headers = {}  # ä¸Šä¼ æ—¶ä¸éœ€è¦Content-Type
            upload_response = requests.put(upload_url, data=f, headers=upload_headers)

            if upload_response.status_code == 200:
                print(f"âœ… æ–‡ä»¶ä¸Šä¼ æˆåŠŸ")
                return batch_id
            else:
                print(f"âŒ æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {upload_response.status_code}")
                return None

    def _poll_task(self, task_id: str, timeout: int = 1800, interval: int = 5) -> dict:
        """è½®è¯¢å•ä¸ªä»»åŠ¡çŠ¶æ€"""
        url = f"https://mineru.net/api/v4/extract/task/{task_id}"
        max_attempts = timeout // interval

        for attempt in range(1, max_attempts + 1):
            print(f"â³ æ£€æŸ¥ä»»åŠ¡çŠ¶æ€ [{attempt}/{max_attempts}]...", end=" ")

            response = requests.get(url, headers=self.headers)
            result = response.json()

            if result.get("code") != 0:
                print(f"æŸ¥è¯¢å¤±è´¥: {result.get('msg')}")
                time.sleep(interval)
                continue

            data = result.get("data", {})
            state = data.get("state", "unknown")

            # æ˜¾ç¤ºè¿›åº¦
            progress = data.get("extract_progress", {})
            if progress:
                extracted = progress.get("extracted_pages", 0)
                total = progress.get("total_pages", 0)
                print(f"{state} ({extracted}/{total}é¡µ)")
            else:
                print(state)

            if state == "done":
                return self._download_result(data)
            elif state in ["failed", "error"]:
                raise ValueError(f"ä»»åŠ¡å¤±è´¥: {data.get('err_msg', 'Unknown error')}")

            time.sleep(interval)

        raise TimeoutError("ä»»åŠ¡è¶…æ—¶")

    def _poll_batch(self, batch_id: str, timeout: int = 1800, interval: int = 5) -> dict:
        """è½®è¯¢æ‰¹é‡ä»»åŠ¡çŠ¶æ€"""
        url = f"https://mineru.net/api/v4/extract-results/batch/{batch_id}"
        max_attempts = timeout // interval

        for attempt in range(1, max_attempts + 1):
            print(f"â³ æ£€æŸ¥æ‰¹é‡ä»»åŠ¡çŠ¶æ€ [{attempt}/{max_attempts}]...", end=" ")

            response = requests.get(url, headers=self.headers)
            result = response.json()

            if result.get("code") != 0:
                print(f"æŸ¥è¯¢å¤±è´¥: {result.get('msg')}")
                time.sleep(interval)
                continue

            data = result.get("data", {})
            extract_results = data.get("extract_result", [])

            if not extract_results:
                print("ç­‰å¾…æ–‡ä»¶å¤„ç†...")
                time.sleep(interval)
                continue

            # æ£€æŸ¥ç¬¬ä¸€ä¸ªæ–‡ä»¶çš„çŠ¶æ€
            file_result = extract_results[0]
            state = file_result.get("state", "unknown")

            # æ˜¾ç¤ºè¿›åº¦
            progress = file_result.get("extract_progress", {})
            if progress:
                extracted = progress.get("extracted_pages", 0)
                total = progress.get("total_pages", 0)
                print(f"{state} ({extracted}/{total}é¡µ)")
            else:
                print(state)

            if state == "done":
                return self._download_result(file_result)
            elif state in ["failed", "error"]:
                raise ValueError(f"ä»»åŠ¡å¤±è´¥: {file_result.get('err_msg', 'Unknown error')}")

            time.sleep(interval)

        raise TimeoutError("ä»»åŠ¡è¶…æ—¶")

    def _download_result(self, data: dict) -> dict:
        """ä¸‹è½½å¹¶è§£æç»“æœ"""
        full_zip_url = data.get("full_zip_url")

        if not full_zip_url:
            raise ValueError("æœªè·å–åˆ°ç»“æœæ–‡ä»¶URL")

        print(f"ğŸ“¥ ä¸‹è½½ç»“æœ: {full_zip_url[:60]}...")

        response = requests.get(full_zip_url)
        if response.status_code != 200:
            raise ValueError(f"ä¸‹è½½å¤±è´¥: {response.status_code}")

        result = {
            "type": "mineru",
            "markdown": "",
            "content_list": [],
            "layout_info": [],
            "images": {},
            "page_mappings": {}
        }

        # è§£å‹å¹¶è¯»å–å†…å®¹
        with zipfile.ZipFile(BytesIO(response.content)) as zf:
            file_list = zf.namelist()
            print(f"ğŸ“¦ è§£å‹æ–‡ä»¶: {file_list}")

            for filename in file_list:
                # è¯»å–markdownæ–‡ä»¶
                if filename.endswith('.md'):
                    result["markdown"] = zf.read(filename).decode('utf-8')
                    print(f"  âœ“ è¯»å–Markdown: {len(result['markdown'])} å­—ç¬¦")

                # è¯»å–content_list.jsonï¼ˆåŒ…å«ä½ç½®ä¿¡æ¯ï¼‰
                elif filename.endswith('content_list.json'):
                    content = zf.read(filename).decode('utf-8')
                    result["content_list"] = json.loads(content)
                    print(f"  âœ“ è¯»å–å†…å®¹åˆ—è¡¨: {len(result['content_list'])} é¡¹")

                # è¯»å–layoutä¿¡æ¯
                elif filename.endswith('layout.json') or filename.endswith('middle.json'):
                    content = zf.read(filename).decode('utf-8')
                    result["layout_info"] = json.loads(content)
                    print(f"  âœ“ è¯»å–å¸ƒå±€ä¿¡æ¯")

                # è¯»å–å›¾ç‰‡ - ä¿å­˜basenameä½œä¸ºé”®å
                elif filename.lower().endswith(('.png', '.jpg', '.jpeg', '.gif')):
                    img_name = os.path.basename(filename)
                    result["images"][img_name] = zf.read(filename)
                    print(f"  âœ“ è¯»å–å›¾ç‰‡: {img_name}")

        # è®¡ç®—æ¯é¡µåæ ‡æ˜ å°„ï¼Œä¾¿äºå‰ç«¯ç²¾ç¡®å®šä½
        result["page_mappings"] = self._build_page_mappings(
            result.get("content_list", []),
            result.get("layout_info")
        )

        print(f"âœ… è§£æå®Œæˆ")
        return result

    def _build_page_mappings(self, content_list, layout_info):
        """æ ¹æ®content_listå’Œlayoutä¿¡æ¯æ„å»ºæ¯é¡µçš„åæ ‡æ˜ å°„"""
        page_stats = {}

        for item in content_list or []:
            bbox = item.get("bbox")
            page_idx = item.get("page_idx")
            if not bbox or page_idx is None:
                continue
            stats = page_stats.setdefault(page_idx, {
                "norm_min_x": bbox[0],
                "norm_max_x": bbox[2],
                "norm_min_y": bbox[1],
                "norm_max_y": bbox[3]
            })
            stats["norm_min_x"] = min(stats["norm_min_x"], bbox[0])
            stats["norm_max_x"] = max(stats["norm_max_x"], bbox[2])
            stats["norm_min_y"] = min(stats["norm_min_y"], bbox[1])
            stats["norm_max_y"] = max(stats["norm_max_y"], bbox[3])

        pdf_info = []
        if isinstance(layout_info, dict):
            pdf_info = layout_info.get("pdf_info", [])
        elif isinstance(layout_info, list):
            pdf_info = layout_info

        for page in pdf_info or []:
            page_idx = page.get("page_idx")
            if page_idx is None:
                continue
            stats = page_stats.setdefault(page_idx, {})
            page_size = page.get("page_size") or []
            if isinstance(page_size, (list, tuple)) and len(page_size) >= 2:
                stats["page_width"], stats["page_height"] = page_size[:2]
            else:
                stats["page_width"] = page.get("width")
                stats["page_height"] = page.get("height")

            x_vals = []
            y_vals = []
            for block in page.get("para_blocks", []):
                bbox = block.get("bbox")
                if bbox:
                    x_vals.extend([bbox[0], bbox[2]])
                    y_vals.extend([bbox[1], bbox[3]])
            for block in page.get("discarded_blocks", []):
                bbox = block.get("bbox")
                if bbox:
                    x_vals.extend([bbox[0], bbox[2]])
                    y_vals.extend([bbox[1], bbox[3]])

            if x_vals:
                stats["actual_min_x"] = min(x_vals)
                stats["actual_max_x"] = max(x_vals)
            elif "page_width" in stats:
                stats.setdefault("actual_min_x", 0)
                stats.setdefault("actual_max_x", stats["page_width"])

            if y_vals:
                stats["actual_min_y"] = min(y_vals)
                stats["actual_max_y"] = max(y_vals)
            elif "page_height" in stats:
                stats.setdefault("actual_min_y", 0)
                stats.setdefault("actual_max_y", stats["page_height"])

        for page_idx, stats in page_stats.items():
            stats.setdefault("norm_min_x", 0)
            stats.setdefault("norm_max_x", stats.get("page_width", 1))
            stats.setdefault("norm_min_y", 0)
            stats.setdefault("norm_max_y", stats.get("page_height", 1))
            stats.setdefault("actual_min_x", 0)
            stats.setdefault("actual_max_x", stats.get("page_width", 0))
            stats.setdefault("actual_min_y", 0)
            stats.setdefault("actual_max_y", stats.get("page_height", 0))
            stats["page_idx"] = page_idx

        return page_stats


if __name__ == "__main__":
    # æµ‹è¯•
    parser = DocumentParser()

    # æµ‹è¯•æœ¬åœ°æ–‡ä»¶ä¸Šä¼ 
    result = parser.parse(file_path="/Users/enithz/Downloads/NC-2023_Sulfate_triple-oxygen-isotope_evidence_confirming_oceanic_oxygenation_570_million_years_ago_2.pdf")

    print(f"\n=== è§£æç»“æœ ===")
    print(f"ç±»å‹: {result['type']}")
    print(f"Markdowné•¿åº¦: {len(result.get('markdown', ''))}")
    print(f"å†…å®¹å—æ•°é‡: {len(result.get('content_list', []))}")
    print(f"å›¾ç‰‡æ•°é‡: {len(result.get('images', {}))}")

    # æ‰“å°å‰500å­—ç¬¦çš„markdown
    if result.get('markdown'):
        print(f"\n=== Markdowné¢„è§ˆ ===")
        print(result['markdown'][:500])
